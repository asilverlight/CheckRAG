{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from flashrag.config import Config\n",
    "from flashrag.utils import get_retriever, get_reranker, get_generator\n",
    "from scripts.pipeline import SequentialPipeline, MultiRAG\n",
    "from modelscope import snapshot_download\n",
    "import ujson\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import ujson\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import BertTokenizer, BertModel, AdamW, BertForSequenceClassification, AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import datetime, time\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "from scripts.utils import load_model, get_dataset\n",
    "\n",
    "config_dict = {\n",
    "    'data_dir': '/data00/yifei_chen/multi_llms_for_CoT/datasets/nq/test.jsonl',\n",
    "    'index_path': '/data00/jiajie_jin/flashrag_indexes/wiki_dpr_100w/e5_flat_inner.index',\n",
    "    #,# ,'/data00/yifei_chen/FlashRAG/examples/quick_start/indexes/e5_Flat.index'\n",
    "    'corpus_path': None,#'/data00/jiajie_jin/flashrag_indexes/wiki_dpr_100w/wiki_dump.jsonl',\n",
    "    # ,# '/data00/yifei_chen/FlashRAG/examples/quick_start/indexes/general_knowledge.jsonl'\n",
    "    'retrieval_cache_path': '/data00/yifei_chen/multi_llms_for_CoT/datasets/wiki_dump/json/default-bfb1d23b390d4c13/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/',\n",
    "    'max_input_len': 1024,\n",
    "    'device': 'cuda:2',\n",
    "    \"framework\": \"hf\",\n",
    "    'do_sample': True,\n",
    "    'max_new_tokens': 1024,\n",
    "    'temperature': 1,\n",
    "    'top_p': 0.7,\n",
    "    'framework': \"hf\",\n",
    "    'generator_model': 'llama3-8B-instruct',# baichuan2-7B-chat\n",
    "    'generator_model_path': \"/data00/LLaMA-3-8b-Instruct/\",\n",
    "    'batch_size': 4,\n",
    "    'result_path': '/data00/yifei_chen/multi_llms_for_CoT/datasets/nq/results/naiveRAG_results.jsonl',\n",
    "    'RAG_type': 'ensemble',\n",
    "    'retrieval_batch_size': 4,\n",
    "    'use_refiner': True,\n",
    "    'use_retrieval_cache': False,\n",
    "    'save_retrieval_cache': False,\n",
    "    # 'device': 'cuda',\n",
    "    # 'cache_dir': \"/data00/yifei_chen/multi_llms_for_CoT/datasets\",\n",
    "}\n",
    "\n",
    "config_inference = {\n",
    "    'ensembler':\n",
    "        {\n",
    "            'model_name': 'llama3-8B-instruct',\n",
    "            'model_path': '/data00/LLaMA-3-8b-Instruct/',\n",
    "            'max_input_len': 1024,\n",
    "            'device': 'cuda:1',\n",
    "            \"framework\": \"hf\",\n",
    "            'type': torch.bfloat16,\n",
    "            'generator_params':\n",
    "                {\n",
    "                    'do_sample': True,\n",
    "                    'max_new_tokens': 1024,\n",
    "                    'temperature': 1,\n",
    "                    'top_p': 0.7,\n",
    "                },\n",
    "            'refiner_topk': 5,\n",
    "            'refiner_pooling_method': 'mean',\n",
    "            'refiner_encode_max_length': 1024,\n",
    "            'refiner_max_input_length': 1024,\n",
    "            'refiner_max_output_length': 1024,\n",
    "        },\n",
    "    'refiner':\n",
    "        {\n",
    "            'model_name': 'llama3-8B-instruct',\n",
    "            'model_path': '/data00/LLaMA-3-8b-Instruct/',\n",
    "            'max_input_len': 1024,\n",
    "            'device': 'cuda',\n",
    "            \"framework\": \"hf\",\n",
    "            'type': torch.bfloat16,\n",
    "            'generator_params':\n",
    "                {\n",
    "                    'do_sample': True,\n",
    "                    'max_new_tokens': 1024,\n",
    "                    'temperature': 1,\n",
    "                    'top_p': 0.7,\n",
    "                },\n",
    "            'refiner_topk': 5,\n",
    "            'refiner_pooling_method': 'mean',\n",
    "            'refiner_encode_max_length': 1024,\n",
    "            'refiner_max_input_length': 1024,\n",
    "            'refiner_max_output_length': 1024,\n",
    "        },\n",
    "    'generators':\n",
    "        [\n",
    "            # {\n",
    "            #     'model_name': 'baichuan2-13b-chat',\n",
    "            #     'model_path': '/data00/yifei_chen/multi_llms_for_CoT/models/baichuan-inc/Baichuan2-13B-Chat',\n",
    "            #     'max_input_len': 1024,\n",
    "            #     'device': 'cuda:2',\n",
    "            #     'data_path': '/data00/yifei_chen/multi_llms_for_CoT/datasets/hotpotqa/sample_small_test.jsonl',\n",
    "            #     \"framework\": \"hf\",\n",
    "            #     'generator_params':\n",
    "            #         {\n",
    "            #             'do_sample': True,\n",
    "            #             'max_new_tokens': 512,\n",
    "            #             'temperature': 1,\n",
    "            #             'top_p': 0.7,\n",
    "            #         }\n",
    "            # },\n",
    "            {\n",
    "                'model_name': 'llama3-8B-instruct',\n",
    "                'model_path': '/data00/LLaMA-3-8b-Instruct/',\n",
    "                'max_input_len': 1024,\n",
    "                'device': 'cuda:0',\n",
    "                'data_path': '/data00/yifei_chen/multi_llms_for_CoT/datasets/hotpotqa/sample_small_test.jsonl',\n",
    "                \"framework\": \"hf\",\n",
    "                'type': torch.bfloat16,\n",
    "                'generator_params':\n",
    "                    {\n",
    "                        'do_sample': True,\n",
    "                        'max_new_tokens': 512,\n",
    "                        'temperature': 1,\n",
    "                        'top_p': 0.7,\n",
    "                    }\n",
    "            },\n",
    "            # {\n",
    "            #     'model_name': 'mistral-7B-instruct-v0.3',\n",
    "            #     'model_path': '/data00/jiajie_jin/model/Mistral-7B-Instruct-v0.3',\n",
    "            #     'max_input_len': 1024,\n",
    "            #     'device': 'cuda:0',\n",
    "            #     'data_path': '/data00/yifei_chen/multi_llms_for_CoT/datasets/hotpotqa/sample_small_test.jsonl',\n",
    "            #     \"framework\": \"hf\",\n",
    "            #     'type': torch.bfloat16,\n",
    "            #     'generator_params':\n",
    "            #         {\n",
    "            #             'do_sample': True,\n",
    "            #             'max_new_tokens': 512,\n",
    "            #             'temperature': 1,\n",
    "            #             'top_p': 0.7,\n",
    "            #         }\n",
    "            # },\n",
    "            # {\n",
    "            #     'model_name': 'qwen2-7B-instruct',\n",
    "            #     'model_path': '/data00/yifei_chen/BERT_classification/models/qwen/Qwen2-7B-Instruct/',\n",
    "            #     'max_input_len': 1024,\n",
    "                \n",
    "            #     'device': 'cuda:0',\n",
    "            #     'data_path': '/data00/yifei_chen/multi_llms_for_CoT/datasets/hotpotqa/sample_small_test.jsonl',\n",
    "            #     \"framework\": \"hf\",\n",
    "            #     'type': torch.bfloat16,\n",
    "            #     'generator_params':\n",
    "            #         {\n",
    "            #             'do_sample': True,\n",
    "            #             'max_new_tokens': 512,\n",
    "            #             'temperature': 1,\n",
    "            #             'top_p': 0.7,\n",
    "            #         }\n",
    "            # },\n",
    "            # {\n",
    "            #     'model_name': 'glm-4-9b-chat',\n",
    "            #     'model_path': '/data00/yifei_chen/multi_llms_for_CoT/models/ZhipuAI/glm-4-9b-chat',\n",
    "            #     'max_input_len': 1024,\n",
    "            #     'device': 'cuda:0',\n",
    "            #     'type': torch.bfloat16,\n",
    "            #     'data_path': '/data00/yifei_chen/multi_llms_for_CoT/datasets/hotpotqa/sample_small_test.jsonl',\n",
    "            #     \"framework\": \"hf\",\n",
    "            #     'generator_params':\n",
    "            #         {\n",
    "            #             'do_sample': True,\n",
    "            #             'max_new_tokens': 512,\n",
    "            #             'temperature': 1,\n",
    "            #             'top_p': 0.7,\n",
    "            #         }\n",
    "            # }\n",
    "        ]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\"/data00/yifei_chen/multi_llms_for_CoT/flashrag/config/basic_config.yaml\", config_dict=config_dict)\n",
    "test_data = get_dataset(config)\n",
    "retriever = get_retriever(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, tokenizers = load_model(config_inference, is_naiverag=False)\n",
    "golden_answers = get_dataset(config, value=\"golden_answers\")\n",
    "\n",
    "# models['judgers'].append(models['decomposer'])\n",
    "# tokenizers['judgers'].append(tokenizers['decomposer'])\n",
    "# models['judgers'] = [models['decomposer']] * 3\n",
    "# tokenizers['judgers'] = [tokenizers['decomposer']] * 3\n",
    "# models = None\n",
    "# tokenizers = None\n",
    "\n",
    "# config_inference['rewritter'] = config_inference['decomposer']\n",
    "# config_inference['generator'] = config_inference['decomposer']\n",
    "# config_inference['modifier'] = config_inference['decomposer']\n",
    "# config_inference['ensembler'] = config_inference['decomposer']\n",
    "\n",
    "pipeline = MultiRAG(config=config, config_inference=config_inference, retriever=retriever, models=models, tokenizers=tokenizers)\n",
    "\n",
    "# pipeline = SequentialPipeline(config=config, config_inference=config_inference, retriever=retriever, models=models, tokenizers=tokenizers)\n",
    "# pipeline.run_naive_RAG(test_data)\n",
    "pipeline.run(test_data, golden_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yifei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
