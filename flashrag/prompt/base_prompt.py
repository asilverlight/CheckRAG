from transformers import AutoTokenizer, AutoConfig


class PromptTemplate:
    placeholders = ["reference", "question"]
    base_system_prompt = (
        "Answer the question based on the given document."
        "Give me the answer and your reasons for the answer on the next line, and do not output any other words."
        "\nThe following are given documents.\n\n{reference}"
    )
    multi_generator_prompt = (
        "Answer the question based on the given document."
        "Only give me the answer and do not output any other words."
        "\nThe following are given documents. Note that the first {num_retriever} documents are returned by the retriever and the last {num_generator} documents are generated by the model.\n\n{reference}"
    )
    base_user_prompt = "Question: {question}"

    def __init__(self, config, system_prompt="", user_prompt="", reference_template=None, enable_chat=True):

        self.config = config
        self.is_openai = config["framework"] == "openai"
        self.multi_model = config['multi_model']
        if not self.is_openai:
            self.generator_path = config["generator_model_path"]
            model_config = AutoConfig.from_pretrained(self.generator_path, trust_remote_code=True)
            # print(model_config)
            model_name = model_config._name_or_path.lower()
            self.is_chat = False
            if "chat" in model_name or "instruct" in model_name:
                self.is_chat = True
                self.tokenizer = AutoTokenizer.from_pretrained(self.generator_path, trust_remote_code=True)
                # print(self.generator_path)
        else:
            self.is_chat = True
            self.enable_chat = True

        if len(system_prompt) == 0 and len(user_prompt) == 0:
            if self.multi_model:
                system_prompt = self.multi_generator_prompt
            else:
                system_prompt = self.base_system_prompt
            user_prompt = self.base_user_prompt
        self.system_prompt = system_prompt
        self.user_prompt = user_prompt
        self.enable_chat = enable_chat
        self.reference_template = reference_template

        # self._check_placeholder()

    def _check_placeholder(self):
        # check placeholder in prompt
        for holder in self.placeholders:
            flag = False
            for prompt in [self.system_prompt, self.user_prompt]:
                if f"{holder}" in prompt:
                    print(f"Find `{holder}` in template")
                    flag = True
                    break
            if not flag and holder != "reference":
                assert False

    def get_string(self, question, retrieval_result=None, formatted_reference=None, previous_gen=None, **params):
        # print(retrieval_result)
        if formatted_reference is None:
            if retrieval_result is not None:
                formatted_reference = self.format_reference(retrieval_result)
            else:
                formatted_reference = ""

        if self.multi_model:
            input_params = {"question": question, "reference": formatted_reference, "num_retriever": self.config['retrieval_topk'], "num_generator": len(self.config['generator_multi'])}
        else:
            input_params = {"question": question, "reference": formatted_reference}
        input_params.update(**params)

        system_prompt = self.system_prompt.format(**input_params)
        user_prompt = self.user_prompt.format(**input_params)

        if self.is_chat and self.enable_chat:
            input = []
            if system_prompt != "":
                input.append({"role": "system", "content": system_prompt})
            if user_prompt != "":
                input.append({"role": "user", "content": user_prompt})
            if self.is_openai:
                for item in input:
                    if item["role"] == "system":
                        item["role"] == "assistant"
            else:
                # print(input)
                if "baichuan" in self.generator_path: # 目前版本的baichuan2还未提供huggingface支持的chat template
                    res_input = ""
                    # for message in input:
                    #     if message['role'] == 'user':
                        #     res_input.append('<reserved_106>') # 将<reserved_106>（即int 195）加入
                        # else:
                        #     res_input.append('<reserved_107>') # 将<reserved_107>（即int 196）加入
                        # res_input.extend(message['content'])
                    message = next(item for item in input if item['role'] == 'system')
                    res_input += '<reserved_106>' + message['content'] + '\n'
                    message = next(item for item in input if item['role'] == 'user')
                    res_input += message['content'] + '<reserved_107>'
                    input = res_input
                else:
                    input = self.tokenizer.apply_chat_template(input, tokenize=False, add_generation_prompt=True)
                # print(input)
        else:
            input = "\n\n".join([prompt for prompt in [system_prompt, user_prompt] if prompt != ""])

        if previous_gen is not None and previous_gen not in ["", " "] and self.is_openai is False:
            input += previous_gen

        return input

    def get_string_with_varying_examplars(
        self,
        question,
        retrieval_result=None,
        formatted_reference=None,
        previous_gen=None,
        examplars=[],
        tokenizer=None,
        max_length=2048,
        **params,
    ):
        """
        Select the maximum number of examplars that can be placed in the prompt
        """

        final_examplars = None
        num = len(examplars)
        while len(examplars) > 0:
            for num in range(len(examplars), 0, -1):
                possible_prompt = self.get_string(
                    question=question,
                    retrieval_result=retrieval_result,
                    formatted_reference=formatted_reference,
                    previous_gen=previous_gen,
                    examplars="\n\n".join(examplars[:num]),
                    **params,
                )

                possible_prompt_tokens = tokenizer.encode(possible_prompt)
                if len(possible_prompt_tokens) <= max_length:
                    final_examplars = examplars[:num]
                    break
            if final_examplars is None:
                examplars = examplars[1:]
            else:
                break
        if final_examplars is None:
            final_examplars = []

        final_prompt = self.get_string(
            question=question,
            retrieval_result=retrieval_result,
            formatted_reference=formatted_reference,
            previous_gen=previous_gen,
            examplars="\n\n".join(final_examplars[:num]),
            **params,
        )

        return final_prompt

    def format_reference(self, retrieval_result):
        format_reference = ""
        for idx, doc_item in enumerate(retrieval_result):
            content = doc_item["contents"]
            title = content.split("\n")[0]
            text = "\n".join(content.split("\n")[1:])
            if self.reference_template is not None:
                format_reference += self.reference_template.format(idx=idx, title=title, text=text)
            else:
                format_reference += f"Doc {idx+1}(Title: {title}) {text}\n"
            print(format_reference)

        return format_reference
